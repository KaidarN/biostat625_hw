---
title: "MLE Estimation"
author: "KN"
date: "2023-11-17"
output: pdf_document
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{Sending Messages With Gmailr}
  %\usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require(AER, quietly = TRUE)) {
  # If not installed, install the AER package
  install.packages("AER")
  # Load the AER package
  library(AER)
} else {
  # If already installed, just load the AER package
  library(AER)
}
library(AER)
```

## Introduction to Maximum Likelihood Estimation (MLE)
MLE is a method of estimating statistical parameters of interest. For a basic regression model we are interested in finding model parameters that maximize the likelihood function. The main goal is to measure how well the model explains the observed data.  

## Linear model
The likelihood function of the linear regression looks as following. 
\begin{equation}
-\frac{1}{2} N \log(2 \pi) - \frac{1}{2} N \log(\hat{\sigma}^2) - \frac{1}{2\sigma^2}(y'y-2y'X\beta+\beta'X'X\beta)
\end{equation}
This is the main function used in the source code. If we have more than values to maximize, then we need a different function for numerical optimization. Hence the MLE function then is passed into r \textit{optim} function to find extreme points of (multivariate functions). The reason for this is complexity with findings an analytical solution from the equation 1. The \textit{optim} function is specified to use Broyden - Fletcher - Goldfrab - Shahho (BFGS). To find the standard errors we need to estimate the variance-covariance matrix as the inverse of the negative Hessian matrix. This is the last peace of code that then combines $\beta$ values and standard errors in one output

## Logistic model
The likelihood function of the logistic regression has the following form. 
\begin{equation}
y \cdot \log\left(\frac{1}{{1 + \exp(-\mu)}}\right) + (1 - y) \cdot \log\left(1 - \frac{1}{{1 + \exp(-\mu)}}\right)
\end{equation}
For the multivariate function the extreme points are calculated using \textit{optim} function and BFGS alogirithm. Standard errors are calculated using the variance-covariance matrix as the inverse of the negative Hessian matrix. The code so far supports only the logit models and will give an error message if the probit model is specified as the argument in the function.


## Example data
To test the functions two data sets are provided. Keep in mind that due to asymptotic properties we need a large sample to get stable estimates. The first data is famous \textit{iris} data set. We use it for fitting multiple linear regression model. In the model Sepal.Length (y) is regressed against Sepal.Width, Petal.Length and Petal.Width (X). For the logistic regression we are using \textit{Affairs} data from AER package. The variables included into analysis are gender (y) religiousness, education, occupation and rating which together represent independent variables (X). Both of the datasets are part of the current package. So please free to replicate the code below.

```{r, echo=F}
ll.models <- function(y, X, model.type) {
      
      if (all(X[,1] == 1))
      {X <- as.matrix(X)}
      else {X <- as.matrix(cbind(1,X))}

      
  if (model.type == 'linear') {
    linmod.llik <- function(theta, y, X) {
      N <- nrow(X) # number of observations
      k <- ncol(X) # number of parameters
      # subset parameter vector
      beta.hat <- theta[1:k]
      gamma.hat <- theta[k + 1]
      # ensure sigma2 to be positive
      sigma2.hat <- exp(gamma.hat)
      # calculate residuals
      e <- y - X %*% beta.hat
      # write down log-likelihood
      llik <- -1/2 * N * log(2 * pi) - 1/2 * N * log(sigma2.hat) - (t(e) %*% e) / (2 * sigma2.hat)
      # return log-likelihood
      return(llik)
    }
    
    inits <- c(0, rep(0,ncol(X))) # define starting values
    # results assignment moved outside of the function definition
    
    results <- optim(inits, linmod.llik, y = y, X = X,
                     control = list(fnscale = -1),
                     method = "BFGS",
                     hessian = TRUE)
    
    Estimate <- results$par[1:ncol(X)]
    Std.Error <- sqrt(diag(-solve(results$hessian)))[1:ncol(X)]
    print(cbind(Estimate, Std.Error))
    
  } else if (model.type == 'logistic') {
    ll.logit <- function(theta, y, X) {
      # theta consists merely of beta (dim is ncol(X))
      beta <- theta[1:ncol(X)]
      # linear predictor; make sure that X is stored as.matrix
      mu <- X %*% beta
      # link function
      p <- 1 / (1 + exp(-mu))
      # log-likelihood
      ll <- y * log(p) + (1 - y) * log(1 - p)
      # sum
      ll <- sum(ll)
      return(ll)
    }
    
    inits <- c(rep(0, ncol(X))) # # define starting values
    # results assignment moved outside of the function definition
    results <- optim(inits, ll.logit, y = y, X = X,
                     control = list(fnscale = -1),
                     method = "BFGS",
                     hessian = TRUE)
    
    Estimate <- results$par
    Std.Error <- sqrt(diag(-solve(results$hessian)))
    print(cbind(Estimate, Std.Error))
    
  } else {
    stop('Please specify the correct model! Currently only Linear and Logistic (link=logit) models are supported')
  }
}
```



## Comparison with existing functions 
To compare the output one can use OLS \textit{lm()} or \textit{glm()} with link \textit{logit}. However keep in mind that OLS and MLE linear regression models will not produce identical results since these are different estimation methods, as shown in the example below. Note that it is important to recode variables into numeric before the analysis, since currently ll.models function does not create reference categories for character and factor type variables.

## Linear regression example and comparison
```{r, echo=T}

data('iris')
attach(iris)

linear_model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)
ll.linear.model <- ll.models (y=Sepal.Length, X=cbind( Sepal.Width, Petal.Length,
                                                       Petal.Width), 
                              model.type = 'linear')

coef_summary <- coef(summary(linear_model))
rownames(coef_summary) <- NULL

# Print selected columns
print(coef_summary[, c(1, 2)]) # OLS results


# Check if coefficients are equal between glm and ll.linear.model
all.equal(coef_summary[, c(1)], ll.linear.model[, 1])
all.equal(coef_summary[, c(2)], ll.linear.model[, 2])
```

## Logistic regression example and comparison
```{r, echo=T}
data("Affairs")
Affairs$gender <- ifelse(Affairs$gender == 'male', 0, 1)
attach(Affairs)
glm_result <- glm(gender ~ religiousness+education+occupation+rating, family=binomial)

ll_models_result <- ll.models(Affairs$gender, X=cbind(religiousness, education,
                                                      occupation, rating), 'logistic')

coef_summary <- coef(summary(glm_result))
rownames(coef_summary) <- NULL

# Print selected columns
print(coef_summary[, c(1, 2)]) # GLM

# Check if coefficients are equal between glm and ll.models
all.equal(coef_summary[, c(1)], ll_models_result[, 1])
all.equal(coef_summary[, c(2)], ll_models_result[, 2])
```


